# ── Core ──────────────────────────────────────────────────────────────────────
torch>=2.4.0
torchaudio>=2.4.0
torchvision>=0.19.0
transformers>=4.48.0
accelerate>=1.1.0
deepspeed>=0.15.0

# ── Data ──────────────────────────────────────────────────────────────────────
datasets>=3.0.0
huggingface-hub>=0.26.0
sentencepiece>=0.2.0
soundfile>=0.12.1
librosa>=0.10.2
Pillow>=10.3.0

# ── Model extras ──────────────────────────────────────────────────────────────
timm>=1.0.9
safetensors>=0.4.5

# ── H100 / BF16 performance ───────────────────────────────────────────────────
# Prebuilt flash-attn wheel (avoids cross-device link build errors).
# Matches: CUDA 12.x, PyTorch 2.8, Python 3.12, cxx11abi=TRUE
https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp312-cp312-linux_x86_64.whl
ninja>=1.11.1

# ── Utilities ────────────────────────────────────────────────────────────────
PyYAML>=6.0.2
tqdm>=4.67.0
packaging>=24.0
